import os
import yaml
import glob
import pandas as pd
import numpy as np
import re
import jieba
import datetime
import logging

# === [1] logging é…ç½® ===
LOG_DIR = 'output'
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, 'selection_analysis_multi_market.log')

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# === [2] è¯»å– run_config.yaml å’Œ version_info ===
with open('config/run_config.yaml', 'r', encoding='utf-8') as f:
    run_config = yaml.safe_load(f)

with open('config/version_info.yaml', 'r', encoding='utf-8') as f:
    version_info = yaml.safe_load(f)

version = version_info.get('version', 'unknown')
print(f"\n==== å½“å‰ç‰ˆæœ¬ï¼š{version} ====\n")
BASE_DIR = os.getcwd()

for market in run_config["markets"]:
    name = market["name"]
    platform = market["platform"]
    country = market["country_code"]
    raw_data_dir = os.path.join(BASE_DIR, market["raw_data_dir"])
    input_glob = os.path.join(raw_data_dir, market["input_glob"])
    output_dir = os.path.join(BASE_DIR, market["output_dir"])
    os.makedirs(output_dir, exist_ok=True)

    market_rule_config_path = market["rule_config"]
    trend_keywords_file = market["trend_keywords_file"]

    logging.info(f"\n=== ğŸš€ å¼€å§‹å¤„ç†å¸‚åœºï¼š{name} ===")

    # 1. è¯»å–æ•°æ®æ–‡ä»¶
    all_files = glob.glob(input_glob)
    if not all_files:
        logging.warning(f"âš ï¸ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼š{input_glob}ï¼Œè·³è¿‡è¯¥å¸‚åœº")
        continue

    df_list = [pd.read_csv(f, encoding='utf-8') for f in all_files]
    store_data = pd.concat(df_list, ignore_index=True)
    store_data = store_data.drop_duplicates(subset=['ç¼–å·'])

    logging.info(f"å…±åŠ è½½ {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œåˆå¹¶åæ€»è¡Œæ•°ï¼š{store_data.shape[0]}")
    # 2. åŠ è½½è¶‹åŠ¿å…³é”®è¯
    if not os.path.exists(trend_keywords_file):
        logging.warning(f"æœªæ‰¾åˆ°è¶‹åŠ¿å…³é”®è¯æ–‡ä»¶ï¼š{trend_keywords_file}ï¼Œä½¿ç”¨é»˜è®¤ fallback")
        trend_keywords_file = 'config/trend_keywords.txt'

    with open(trend_keywords_file, 'r', encoding='utf-8') as f:
        trend_keywords = [line.strip() for line in f if line.strip()]

    logging.info(f"å·²åŠ è½½è¶‹åŠ¿å…³é”®è¯ï¼š{trend_keywords_file}ï¼Œå…± {len(trend_keywords)} ä¸ª")

    # 3. åŠ è½½å¸‚åœºè§„åˆ™ï¼ˆCSVï¼‰
    if os.path.exists(market_rule_config_path):
        market_rule_df = pd.read_csv(market_rule_config_path, encoding='utf-8')
        logging.info(f"å·²åŠ è½½å¸‚åœºè§„åˆ™æ–‡ä»¶ï¼š{market_rule_config_path}")
    else:
        logging.warning(f"âš ï¸ æœªæ‰¾åˆ°å¸‚åœºè§„åˆ™æ–‡ä»¶ï¼š{market_rule_config_path}ï¼Œå°†è·³è¿‡è¯¥å¸‚åœº")
        continue

    # 4. å­—æ®µæ¸…æ´—ä¸å¡«å……
    drop_cols = ['å›¾ç‰‡', 'åŒ…æsku', 'å¥—é¤', 'å·²ä¸Šæ¶åº—é“º', 'æœ€è¿‘è°ƒä»·', 'æ˜¯å¦ä¸ºæ–°å“', 'æ˜¯å¦ä¸Šæ¶åº—é“º', 'åº—é“º', 'äº§å“ç´ æåŒ…']
    store_data.drop(columns=[c for c in drop_cols if c in store_data.columns], inplace=True, errors='ignore')

    fill_cols = ['ä½“ç§¯', 'é‡é‡', 'ä½“ç§¯é‡', 'åˆä½œå•†æ´»åŠ¨æœ€ä½å”®ä»·å¤–å¸', 'ç®±å­è§„æ ¼', 'å…³é”®è¯', 'ä¸»æ¨æ–¹å‘']
    for col in fill_cols:
        if col in store_data.columns:
            if store_data[col].dtype == 'O':
                store_data[col] = store_data[col].fillna('')
            else:
                store_data[col] = pd.to_numeric(store_data[col], errors='coerce').fillna(0)
    # 5. å¹³å°å”®ä»·æå–ï¼ˆShopee å’Œ Lazadaï¼‰
    def extract_price(val, platform_name='Shopee'):
        if pd.isnull(val):
            return np.nan
        m = re.search(fr'{platform_name}[ï¼š:]\s*(\d+(\.\d+)?)', str(val), re.IGNORECASE)
        if m:
            return float(m.group(1))
        return np.nan

    store_data['Shopeeä»·æ ¼'] = store_data['åˆä½œå•†æœ€ä½å”®ä»·å¤–å¸'].apply(lambda x: extract_price(x, 'Shopee'))
    store_data['Lazadaä»·æ ¼'] = store_data['åˆä½œå•†æœ€ä½å”®ä»·å¤–å¸'].apply(lambda x: extract_price(x, 'Lazada'))

    # 6. æˆæœ¬å’Œ Shopee æ¯›åˆ©ç‡è®¡ç®—
    for col in run_config['numeric_cols'] + ['Shopeeä»·æ ¼', 'Lazadaä»·æ ¼']:
        store_data[col] = pd.to_numeric(store_data[col], errors='coerce').fillna(0)

    store_data['æ€»æˆæœ¬'] = (
        store_data.get('æœ¬æœŸé‡‡è´­ä»·', 0) +
        store_data.get('å›½å†…è¿è´¹', 0) +
        store_data.get('å›½é™…è¿è´¹', 0) +
        store_data.get('ä»“åº“æ“ä½œè´¹', 0)
    )

    store_data['Shopeeæ¯›åˆ©ç‡'] = np.where(
        store_data['Shopeeä»·æ ¼'] > 0,
        (store_data['Shopeeä»·æ ¼'] - store_data['æ€»æˆæœ¬']) / store_data['Shopeeä»·æ ¼'],
        np.nan
    )

    # 7. å‰”é™¤æ¯›åˆ©ç‡ < -10% çš„å¼‚å¸¸è¡Œ
    abnormal_mask = store_data['Shopeeæ¯›åˆ©ç‡'] < -0.1
    removed_rows = store_data[abnormal_mask]
    store_data = store_data[~abnormal_mask]

    removed_path = os.path.join(output_dir, 'removed_rows.csv')
    removed_rows.to_csv(removed_path, index=False, encoding='utf_8_sig')
    logging.info(f"å¼‚å¸¸æ¯›åˆ©ç‡å‰”é™¤è¡Œæ•°ï¼š{removed_rows.shape[0]}ï¼Œä¿å­˜è‡³ï¼š{removed_path}")
    # 8. æ´»è·ƒäº§å“ç­›é€‰
    status_config_path = os.path.join('config', 'status_config.yaml')
    if os.path.exists(status_config_path):
        with open(status_config_path, 'r', encoding='utf-8') as f:
            status_config = yaml.safe_load(f)
        status_keep = status_config.get('status_keep', [])
    else:
        status_keep = ['æ­£å¸¸å¤‡è´§', 'ç¤¼å“', 'æ¸…åº“å­˜']

    active = store_data[store_data['äº§å“çŠ¶æ€'].isin(status_keep)].copy()
    logging.info(f"ç­›é€‰æ´»è·ƒäº§å“åå‰©ä½™ï¼š{active.shape[0]} è¡Œ")

    # 9. åŒ¹é…åº¦è®¡ç®—
    def calculate_match(text, keywords):
        if pd.isnull(text):
            return 0
        text = str(text)
        words = list(jieba.cut(text))
        return sum(1 for word in words if word in keywords)

    active['å…³é”®è¯'] = active['å…³é”®è¯'].fillna(active['ä¸­æ–‡åç§°'])
    active['åŒ¹é…åº¦'] = active['å…³é”®è¯'].apply(lambda x: calculate_match(x, trend_keywords))

    # 10. SKUè¯„åˆ†å’Œåˆ†ç±»
    active['SKUè¯„åˆ†'] = 100

    def classify_status(row):
        if row['äº§å“çŠ¶æ€'] == 'æ­£å¸¸å¤‡è´§':
            return 'ä¸»åŠ›ä¸Šæ–°'
        elif row['äº§å“çŠ¶æ€'] == 'æ¸…åº“å­˜':
            return 'ç¤¼åŒ…SKUï¼ˆç¤¼å“å¯ç”¨ï¼‰'
        elif row['äº§å“çŠ¶æ€'] == 'ç¤¼å“':
            return 'ç¤¼åŒ…SKUï¼ˆç¤¼å“ä¸“ç”¨ï¼‰'
        else:
            return 'äººå·¥ç¡®è®¤'

    active['äº§å“çŠ¶æ€åˆ†ç±»'] = active.apply(classify_status, axis=1)

    def gen_ope_advice(row):
        if row['äº§å“çŠ¶æ€'] == 'æ­£å¸¸å¤‡è´§':
            return 'æ¨èä¸Šæ¶ï¼ˆä¸Šæ¶ä»· â‰¥ æœ€ä½ä»·ï¼Œç¦æ­¢ä½ä»·è¿è§„ï¼‰'
        elif row['äº§å“çŠ¶æ€'] == 'æ¸…åº“å­˜':
            return 'æš‚ä¸æ¨èä¸Šæ¶ï¼ˆå½“å‰é˜¶æ®µä¸åšç¤¼åŒ…ä¿ƒé”€ï¼‰'
        else:
            return 'å¯ä¸Šæ¶ï¼ˆéœ€æ‰‹åŠ¨ç¡®è®¤æœ€ä½ä»·åˆè§„ï¼‰'

    active['è¿è¥ä¸Šæ¶å»ºè®®'] = active.apply(gen_ope_advice, axis=1)

    # 11. ä¸Šæ–°ä¼˜å…ˆçº§è¯„åˆ†
    active['Shopeeæ¯›åˆ©ç‡_clip'] = active['Shopeeæ¯›åˆ©ç‡'].clip(lower=0, upper=1)
    active['æ–°å“ä¼˜å…ˆæƒé‡'] = active['ç¼–å·'] / active['ç¼–å·'].max()

    weights = run_config['priority_weights']
    active['ä¸Šæ–°ä¼˜å…ˆçº§'] = (
        active['åŒ¹é…åº¦'] * weights['matching_degree'] +
        active['Shopeeæ¯›åˆ©ç‡_clip'] * weights['shopee_margin'] +
        active['æ–°å“ä¼˜å…ˆæƒé‡'] * weights['new_product_weight']
    )
    # 12. å¯¼å‡ºæ’åºç»“æœ
    priority_products = active.sort_values(
        by='ä¸Šæ–°ä¼˜å…ˆçº§',
        ascending=False
    )[
        ['ç¼–å·', 'sku', 'ä¸­æ–‡åç§°', 'äº§å“ç±»åˆ«', 'è§„æ ¼', 'Shopeeä»·æ ¼',
         'äº§å“çŠ¶æ€', 'äº§å“çŠ¶æ€åˆ†ç±»', 'è¿è¥ä¸Šæ¶å»ºè®®',
         'åŒ¹é…åº¦', 'Shopeeæ¯›åˆ©ç‡', 'SKUè¯„åˆ†', 'æ–°å“ä¼˜å…ˆæƒé‡', 'ä¸Šæ–°ä¼˜å…ˆçº§']
    ]

    output_path = os.path.join(output_dir, f'priority_products_{country}.csv')
    priority_products.to_csv(output_path, index=False, encoding='utf_8_sig')
    logging.info(f"âœ… å·²å¯¼å‡ºæ’åºç»“æœï¼š{output_path}")

    # 13. ç”Ÿæˆ SKU æ–‡ä»¶å¤¹ç»“æ„
    export_dir = os.path.join(BASE_DIR, f'output/product_manage_{country}')
    os.makedirs(export_dir, exist_ok=True)

    def sanitize_filename(name):
        return re.sub(r'[\\/:*?"<>|]', '_', str(name)).strip()

    for idx, row in priority_products.reset_index(drop=True).iterrows():
        sku_index = f"{idx + 1:03d}"  # ä¸‰ä½æ•°ç¼–å·
        folder_name = f"{sku_index}_{row['ç¼–å·']}_{sanitize_filename(row['sku'])}_" \
                      f"{sanitize_filename(row['äº§å“çŠ¶æ€'])}_{sanitize_filename(row['äº§å“ç±»åˆ«'])}_" \
                      f"{sanitize_filename(row['ä¸­æ–‡åç§°'])}"

        folder_path = os.path.join(export_dir, folder_name)
        os.makedirs(folder_path, exist_ok=True)

    logging.info(f"ğŸ“ å·²ç”Ÿæˆ {len(priority_products)} ä¸ª SKU æ–‡ä»¶å¤¹è‡³ï¼š{export_dir}")
