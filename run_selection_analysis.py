# run_selection_analysis.py
<<<<<<< HEAD
# äº§å“é€‰å“åˆ†æå¢å¼ºç‰ˆ V1.4.1
# ğŸš€ æ›´æ–°ç‚¹ï¼š
# - æ–°å¢ trend_keywords_file é…ç½®ï¼Œæ”¯æŒå¤šå¸‚åœºè¶‹åŠ¿è¯æ–‡ä»¶
# - run_config.yaml é…ç½® trend_keywords_file + market_code
# - ä¼˜åŒ– logging + ç‰ˆæœ¬ç»“æ„
=======
# äº§å“é€‰å“åˆ†æå¢å¼ºç‰ˆ V1.4.0
# ğŸš€ æ›´æ–°ç‚¹ï¼š
# - æ–°å¢ market_rule_config_TH.yaml è§„èŒƒé…ç½®æ”¯æŒ
# - run_selection_analysis.py å¢åŠ å¸‚åœºè§„åˆ™åŠ è½½ stub
# - ç»´æŒç‰ˆæœ¬ä¿¡æ¯ / ç»“æ„æ¸…æ™°å®šä½
>>>>>>> 13c0f29f4588310ec1ec962f7b6a40d8c4f31b53

import os
import yaml
import glob
import pandas as pd
import numpy as np
import re
import jieba
import datetime
import logging

# === [1] logging é…ç½® ===
LOG_DIR = 'output'
os.makedirs(LOG_DIR, exist_ok=True)
LOG_FILE = os.path.join(LOG_DIR, 'selection_analysis.log')

<<<<<<< HEAD
=======
# utf-8 handler
>>>>>>> 13c0f29f4588310ec1ec962f7b6a40d8c4f31b53
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE, encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# === [2] è¯»å– version_info.yaml ===
with open('config/version_info.yaml', 'r', encoding='utf-8') as f:
    version_info = yaml.safe_load(f)

version = version_info.get('version', 'unknown')
compatibility = version_info.get('compatibility', 'unknown')
update_date = version_info.get('update_date', 'unknown')
update_log = version_info.get('update_log', [])

# æ‰“å°ç‰ˆæœ¬ä¿¡æ¯
print(f"\n==== å½“å‰ç‰ˆæœ¬ä¿¡æ¯ ====")
print(f"ç‰ˆæœ¬å·ï¼š{version} ({compatibility})")
print(f"æè¿°ï¼š{version_info.get('description', 'No description')}")
print(f"æ›´æ–°æ—¥æœŸï¼š{update_date}")
print("æ›´æ–°æ—¥å¿—ï¼š")
for log_entry in update_log:
    print(f" - {log_entry}")
print("=" * 40)

# logging ç‰ˆæœ¬ä¿¡æ¯
logging.info(f"Version: {version} | Compatibility: {compatibility} | Update Date: {update_date}")
logging.info("Update Log:")
for log_entry in update_log:
    logging.info(f"- {log_entry}")
<<<<<<< HEAD
=======

>>>>>>> 13c0f29f4588310ec1ec962f7b6a40d8c4f31b53
# === [3] è¯»å– run_config.yaml ===
with open('config/run_config.yaml', 'r', encoding='utf-8') as f:
    run_config = yaml.safe_load(f)

BASE_DIR = os.getcwd()
RAW_DATA_DIR = os.path.join(BASE_DIR, run_config['raw_data_dir'])
INPUT_GLOB = os.path.join(RAW_DATA_DIR, run_config['input_glob'])
OUTPUT_DIR = os.path.join(BASE_DIR, run_config['output_dir'])
NUMERIC_COLS = run_config['numeric_cols']

<<<<<<< HEAD
# æ–°å¢
MARKET_CODE = run_config.get('market_code', 'TH')
market_rule_config_path = run_config.get('market_rule_config_path', f'config/market_rule_config_{MARKET_CODE}.yaml')
trend_keywords_file = run_config.get('trend_keywords_file', f'config/trend_keywords_{MARKET_CODE}.txt')

=======
>>>>>>> 13c0f29f4588310ec1ec962f7b6a40d8c4f31b53
os.makedirs(OUTPUT_DIR, exist_ok=True)

# === [4] è¯»å– status_config.yaml ===
with open('config/status_config.yaml', 'r', encoding='utf-8') as f:
    status_config = yaml.safe_load(f)

status_keep = status_config['status_keep']

<<<<<<< HEAD
# === [5] è¯»å– trend_keywords_file ===
if not os.path.exists(trend_keywords_file):
    logging.warning(f"æœªæ‰¾åˆ°è¶‹åŠ¿è¯æ–‡ä»¶ {trend_keywords_file}ï¼Œä½¿ç”¨ fallback é€šç”¨è¶‹åŠ¿è¯ trend_keywords.txt")
    trend_keywords_file = 'config/trend_keywords.txt'

with open(trend_keywords_file, 'r', encoding='utf-8') as f:
    trend_keywords = [line.strip() for line in f if line.strip()]

logging.info(f"è¶‹åŠ¿å…³é”®è¯å·²åŠ è½½: {trend_keywords_file} ï¼ˆå…± {len(trend_keywords)} ä¸ªè¯ï¼‰")

# === [6] è¯»å– market_rule_config.yaml ===
with open(market_rule_config_path, 'r', encoding='utf-8') as f:
    market_rule_config = yaml.safe_load(f)

logging.info(f"å¸‚åœºè§„åˆ™é…ç½®å·²åŠ è½½: {market_rule_config_path}")
=======
# === [5] è¯»å– trend_keywords.txt ===
with open('config/trend_keywords.txt', 'r', encoding='utf-8') as f:
    trend_keywords = [line.strip() for line in f if line.strip()]

# === [6] è¯»å– market_rule_config_TH.yaml ===
with open('config/market_rule_config_TH.yaml', 'r', encoding='utf-8') as f:
    market_rule_config = yaml.safe_load(f)

logging.info("å¸‚åœºè§„åˆ™é…ç½®å·²åŠ è½½ (market_rule_config_TH.yaml)")
>>>>>>> 13c0f29f4588310ec1ec962f7b6a40d8c4f31b53
logging.info(f"å½“å‰å¹³å°è§„åˆ™æ›´æ–°æ—¥æœŸ: {market_rule_config['market_rule']['meta']['update_date']}")

# === [7] æ•°æ®åŠ è½½ ===
all_files = glob.glob(INPUT_GLOB)
df_list = [pd.read_csv(f, encoding='utf-8') for f in all_files]
store_data = pd.concat(df_list, ignore_index=True)
store_data = store_data.drop_duplicates(subset=['ç¼–å·'])

print(f"\nå…±åŠ è½½ {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œå»é‡åæ€»è¡Œæ•°ï¼š{store_data.shape[0]}")
logging.info(f"å…±åŠ è½½ {len(all_files)} ä¸ªæ–‡ä»¶ï¼Œå»é‡åæ€»è¡Œæ•°ï¼š{store_data.shape[0]}")

# === [8] ç¼ºå¤±å€¼å¤„ç† & å­—æ®µå¤„ç† ===
drop_cols = ['å›¾ç‰‡', 'åŒ…æsku', 'å¥—é¤', 'å·²ä¸Šæ¶åº—é“º', 'æœ€è¿‘è°ƒä»·', 'æ˜¯å¦ä¸ºæ–°å“', 'æ˜¯å¦ä¸Šæ¶åº—é“º', 'åº—é“º', 'äº§å“ç´ æåŒ…']
store_data.drop(columns=[c for c in drop_cols if c in store_data.columns], inplace=True, errors='ignore')

fill_cols = ['ä½“ç§¯', 'é‡é‡', 'ä½“ç§¯é‡', 'åˆä½œå•†æ´»åŠ¨æœ€ä½å”®ä»·å¤–å¸', 'ç®±å­è§„æ ¼', 'å…³é”®è¯', 'ä¸»æ¨æ–¹å‘']
for col in fill_cols:
    if col in store_data.columns:
        if store_data[col].dtype == 'O':
            store_data[col] = store_data[col].fillna('')
        else:
            store_data[col] = pd.to_numeric(store_data[col], errors='coerce').fillna(0)

# === [9] æå–å¹³å°å”®ä»· ===
def extract_price(val, platform='Shopee'):
    if pd.isnull(val):
        return np.nan
    m = re.search(fr'{platform}[ï¼š:]\s*(\d+(\.\d+)?)', str(val), re.IGNORECASE)
    if m:
        return float(m.group(1))
    return np.nan

store_data['Shopeeä»·æ ¼'] = store_data['åˆä½œå•†æœ€ä½å”®ä»·å¤–å¸'].apply(lambda x: extract_price(x, 'Shopee'))
store_data['Lazadaä»·æ ¼'] = store_data['åˆä½œå•†æœ€ä½å”®ä»·å¤–å¸'].apply(lambda x: extract_price(x, 'Lazada'))

# === [10] æˆæœ¬&æ¯›åˆ©ç‡ ===
for col in NUMERIC_COLS + ['Shopeeä»·æ ¼', 'Lazadaä»·æ ¼']:
    store_data[col] = pd.to_numeric(store_data[col], errors='coerce').fillna(0)

store_data['æ€»æˆæœ¬'] = (
    store_data['æœ¬æœŸé‡‡è´­ä»·'] +
    store_data['å›½å†…è¿è´¹'] +
    store_data['å›½é™…è¿è´¹'] +
    store_data['ä»“åº“æ“ä½œè´¹']
)

store_data['Shopeeæ¯›åˆ©ç‡'] = np.where(
    store_data['Shopeeä»·æ ¼'] > 0,
    (store_data['Shopeeä»·æ ¼'] - store_data['æ€»æˆæœ¬']) / store_data['Shopeeä»·æ ¼'],
    np.nan
)

# === [11] å¼‚å¸¸ Shopeeæ¯›åˆ©ç‡ è¡Œå‰”é™¤ ===
abnormal_mask = store_data['Shopeeæ¯›åˆ©ç‡'] < -0.1
removed_rows = store_data[abnormal_mask]
store_data = store_data[~abnormal_mask]
removed_rows.to_csv(os.path.join(OUTPUT_DIR, 'removed_rows.csv'), index=False, encoding='utf_8_sig')

logging.info(f"å‰”é™¤å¼‚å¸¸ Shopeeæ¯›åˆ©ç‡ è¡Œæ•°ï¼š{removed_rows.shape[0]}")

# === æ´»è·ƒäº§å“ç­›é€‰ ===
active = store_data[store_data['äº§å“çŠ¶æ€'].isin(status_keep)].copy()
print("æ´»è·ƒäº§å“æ•°é‡ï¼š", active.shape[0])
logging.info(f"æ´»è·ƒäº§å“æ•°é‡ï¼š{active.shape[0]}")

# === åŒ¹é…åº¦è®¡ç®— ===
def calculate_match(text, keywords):
    if pd.isnull(text):
        return 0
    text = str(text)
    words = list(jieba.cut(text))
    return sum(1 for word in words if word in keywords)

active['å…³é”®è¯'] = active['å…³é”®è¯'].fillna(active['ä¸­æ–‡åç§°'])
active['åŒ¹é…åº¦'] = active['å…³é”®è¯'].apply(lambda x: calculate_match(x, trend_keywords))

# === [12] SKUè¯„åˆ† ä¿®æ­£ ===
active['SKUè¯„åˆ†'] = 100

# === [13] äº§å“çŠ¶æ€åˆ†ç±» ===
def classify_status(row):
    if row['äº§å“çŠ¶æ€'] == 'æ­£å¸¸å¤‡è´§':
        return 'ä¸»åŠ›ä¸Šæ–°'
    elif row['äº§å“çŠ¶æ€'] == 'æ¸…åº“å­˜':
        return 'ç¤¼åŒ…SKUï¼ˆç¤¼å“å¯ç”¨ï¼‰'
    elif row['äº§å“çŠ¶æ€'] == 'ç¤¼å“':
        return 'ç¤¼åŒ…SKUï¼ˆç¤¼å“ä¸“ç”¨ï¼‰'
    else:
        return 'äººå·¥ç¡®è®¤'

active['äº§å“çŠ¶æ€åˆ†ç±»'] = active.apply(classify_status, axis=1)

# === [13] è¿è¥ä¸Šæ¶å»ºè®® ===
def gen_ope_advice(row):
    if row['äº§å“çŠ¶æ€'] == 'æ­£å¸¸å¤‡è´§':
        return 'æ¨èä¸Šæ¶ï¼ˆä¸Šæ¶ä»· â‰¥ æœ€ä½ä»·ï¼Œç¦æ­¢ä½ä»·è¿è§„ï¼‰'
    elif row['äº§å“çŠ¶æ€'] == 'æ¸…åº“å­˜':
        return 'æš‚ä¸æ¨èä¸Šæ¶ï¼ˆå½“å‰é˜¶æ®µä¸åšç¤¼åŒ…ä¿ƒé”€ï¼‰'
    else:
        return 'å¯ä¸Šæ¶ï¼ˆéœ€æ‰‹åŠ¨ç¡®è®¤æœ€ä½ä»·åˆè§„ï¼‰'

active['è¿è¥ä¸Šæ¶å»ºè®®'] = active.apply(gen_ope_advice, axis=1)

# === [14] ä¸Šæ–°ä¼˜å…ˆçº§è®¡ç®— ===
active['Shopeeæ¯›åˆ©ç‡_clip'] = active['Shopeeæ¯›åˆ©ç‡'].clip(lower=0, upper=1)
active['æ–°å“ä¼˜å…ˆæƒé‡'] = active['ç¼–å·'] / active['ç¼–å·'].max()

active['ä¸Šæ–°ä¼˜å…ˆçº§'] = (
    active['åŒ¹é…åº¦'] * 5 +
    active['Shopeeæ¯›åˆ©ç‡_clip'] * 100 +
    active['æ–°å“ä¼˜å…ˆæƒé‡'] * 10
)

# === æ’åº&å¯¼å‡º ===
priority_products = active.sort_values(
    by='ä¸Šæ–°ä¼˜å…ˆçº§',
    ascending=False
<<<<<<< HEAD
)[['ç¼–å·', 'sku', 'ä¸­æ–‡åç§°', 'äº§å“ç±»åˆ«', 'è§„æ ¼', 'Shopeeä»·æ ¼', 'äº§å“çŠ¶æ€', 'äº§å“çŠ¶æ€åˆ†ç±»', 'è¿è¥ä¸Šæ¶å»ºè®®',
=======
)[['ç¼–å·', 'sku', 'ä¸­æ–‡åç§°', 'äº§å“ç±»åˆ«', 'äº§å“çŠ¶æ€', 'äº§å“çŠ¶æ€åˆ†ç±»', 'è¿è¥ä¸Šæ¶å»ºè®®',
>>>>>>> 13c0f29f4588310ec1ec962f7b6a40d8c4f31b53
   'åŒ¹é…åº¦', 'Shopeeæ¯›åˆ©ç‡', 'SKUè¯„åˆ†', 'æ–°å“ä¼˜å…ˆæƒé‡', 'ä¸Šæ–°ä¼˜å…ˆçº§']]

priority_products.to_csv(
    os.path.join(OUTPUT_DIR, 'priority_products.csv'),
    index=False,
    encoding='utf_8_sig'
)

<<<<<<< HEAD
# === [15] ç”Ÿæˆ SKU æ–‡ä»¶å¤¹ç»“æ„ ===
EXPORT_DIR = os.path.join(BASE_DIR, f'output/product_manage_{MARKET_CODE}')
os.makedirs(EXPORT_DIR, exist_ok=True)

# æ¸…ç†éæ³•å­—ç¬¦å‡½æ•°ï¼ˆç”¨äºæ–‡ä»¶å¤¹åï¼‰
def sanitize_filename(name):
    return re.sub(r'[\/:*?"<>|]', '_', str(name)).strip()

for idx, row in priority_products.reset_index(drop=True).iterrows():
    sku_index = f"{idx + 1:03d}"  # ä¸‰ä½æ•°ç¼–å·
    folder_name = f"{sku_index}_{row['ç¼–å·']}_{sanitize_filename(row['sku'])}_" \
                  f"{sanitize_filename(row['äº§å“çŠ¶æ€'])}_{sanitize_filename(row['äº§å“ç±»åˆ«'])}_" \
                  f"{sanitize_filename(row['ä¸­æ–‡åç§°'])}"

    folder_path = os.path.join(EXPORT_DIR, folder_name)
    os.makedirs(folder_path, exist_ok=True)

logging.info(f"å·²ç”Ÿæˆ {len(priority_products)} ä¸ª SKU æ–‡ä»¶å¤¹è‡³ï¼š{EXPORT_DIR}")
print(f"å·²ç”Ÿæˆ {len(priority_products)} ä¸ª SKU æ–‡ä»¶å¤¹è‡³ï¼š{EXPORT_DIR}")
=======
with pd.ExcelWriter(os.path.join(OUTPUT_DIR, 'product_selection_analysis_result.xlsx'), engine='openpyxl') as writer:
    priority_products.to_excel(writer, sheet_name='ä¼˜å…ˆä¸Šæ–°äº§å“', index=False)
    active.to_excel(writer, sheet_name='æ´»è·ƒäº§å“å…¨è¡¨', index=False)
>>>>>>> 13c0f29f4588310ec1ec962f7b6a40d8c4f31b53

# === version_info.yaml æ›´æ–° ===
try:
    version_info['last_run_date'] = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    with open('config/version_info.yaml', 'w', encoding='utf-8') as f:
        yaml.safe_dump(version_info, f, allow_unicode=True)
    logging.info(f"å·²æ›´æ–° version_info.yaml last_run_date â†’ {version_info['last_run_date']}")
except Exception as e:
    logging.error(f"âš ï¸ æ›´æ–° version_info.yaml å¤±è´¥ï¼š{e}")

# === ç»“æŸ ===
print("\nâœ… æ•°æ®å¤„ç†å®Œæ¯•ï¼Œå¢å¼ºç‰ˆç»“æœå·²è¾“å‡ºåˆ°ï¼š", OUTPUT_DIR)
logging.info("âœ… æ•°æ®å¤„ç†å®Œæ¯•ï¼Œå¢å¼ºç‰ˆç»“æœå·²è¾“å‡º")
